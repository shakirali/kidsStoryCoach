"""Integration tests for Title Generator Agent evaluation.

This test module evaluates titles generated by the Title Generator Agent
using the judge agent to ensure quality standards are met.
"""

import pytest
import random
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from agents.sub_agents.title_generator.agent import title_generator_agent
from agents.evaluation.helpers import evaluate_with_judge, call_agent_async
from agents.evaluation.prompts import instruction_title_judge
from agents.evaluation.judge_agent import judge_agent


@pytest.fixture
def session_id():
    """Create a random session id."""
    return str(random.randint(1000000000, 9999999999))


@pytest.fixture
def user_id():
    """Create a random user id."""
    return str(random.randint(1000000000, 9999999999))


@pytest.fixture
def title_app_name():
    return "title_test"


@pytest.fixture
def judge_app_name():
    return "judge_test"


@pytest.fixture
def session_service():
    """Create a session service for tests."""
    return InMemorySessionService()


@pytest.fixture
def title_runner(session_service):
    """Create a runner for the title generator agent."""
    return Runner(
        app_name="title_test",
        agent=title_generator_agent,
        session_service=session_service
    )


@pytest.fixture
def judge_runner(session_service):
    """Create a runner for the judge agent."""
    return Runner(
        app_name="judge_test",
        agent=judge_agent,
        session_service=session_service
    )


# Test cases: (story_concept, author_name, expected_min_score)
TEST_CASES = [
    ("A brave knight saves a dragon from an evil wizard", "Emma", 3.5),
    ("A magical forest adventure with talking animals", "Lucas", 3.5),
    ("A robot learns to dance", "Sophia", 3.5),
    ("A space adventure to find a new planet", "Oliver", 3.5),
    ("A friendly monster who helps children", "Ava", 3.5),
]


@pytest.mark.asyncio
@pytest.mark.parametrize("story_concept,author_name,min_score", TEST_CASES)
async def test_title_generator_produces_quality_titles(
    title_runner,
    judge_runner,
    session_service,
    session_id,
    user_id,
    title_app_name,
    judge_app_name,
    story_concept,
    author_name,
    min_score
):
    """Test that Title Generator Agent produces titles that meet quality standards."""
    # Step 1: Generate title using Title Generator Agent
    title_session = await session_service.create_session(
        session_id=f"{session_id}_title",
        user_id=user_id,
        app_name=title_app_name
    )
    
    title_input = f"author_name: {author_name}\nstory_concept: {story_concept}"
    generated_title = await call_agent_async(
        query=title_input,
        runner=title_runner,
        user_id=user_id,
        session_id=title_session.id
    )
    
    # Clean up the title (remove any extra whitespace)
    generated_title = generated_title.strip()
    
    # Verify we got a title
    assert generated_title is not None, "Title generator should return a title"
    assert len(generated_title) > 0, "Title should not be empty"
    assert len(generated_title) <= 100, f"Title should be reasonable length, got {len(generated_title)} chars"
    
    # Step 2: Evaluate with judge agent
    evaluation_input = f"story_concept: {story_concept}\ngenerated_title: {generated_title}"
    
    judge_session_id = f"{session_id}_judge"
    evaluation_result = await evaluate_with_judge(
        judge_instruction=instruction_title_judge(),
        evaluation_input=evaluation_input,
        session_service=session_service,
        runner=judge_runner,
        user_id=user_id,
        session_id=judge_session_id,
        app_name=judge_app_name,
        expected_min_score=min_score
    )
    
    # Step 3: Verify evaluation results
    scores = evaluation_result["scores"]
    overall_score = evaluation_result["overall_score"]
    
    # Verify all scores are in valid range
    for score_name, score_value in scores.items():
        assert isinstance(score_value, int), f"{score_name} should be an integer"
        assert 1 <= score_value <= 5, f"{score_name} should be between 1 and 5, got {score_value}"
    
    # Verify overall score meets minimum
    assert overall_score >= min_score, \
        f"Title '{generated_title}' scored {overall_score}, below minimum {min_score}. " \
        f"Scores: {scores}, Reasoning: {evaluation_result.get('reasoning', 'N/A')}"
    
    # Verify pass status
    assert evaluation_result["pass"] is True, \
        f"Title '{generated_title}' should pass evaluation. " \
        f"Overall score: {overall_score}, Reasoning: {evaluation_result.get('reasoning', 'N/A')}"
    
    print(f"\nTitle: '{generated_title}'")
    print(f"  Story Concept: {story_concept}")
    print(f"  Author: {author_name}")
    print(f"  Scores: {scores}")
    print(f"  Overall: {overall_score}")
    print(f"  Reasoning: {evaluation_result.get('reasoning', 'N/A')}")


@pytest.mark.asyncio
async def test_title_generator_multiple_titles_consistency(
    title_runner,
    judge_runner,
    session_service,
    session_id,
    user_id,
    title_app_name,
    judge_app_name
):
    """Test that Title Generator produces consistently good titles across multiple runs."""
    story_concept = "A magical unicorn adventure"
    author_name = "Lily"
    
    titles = []
    scores = []
    
    # Generate 3 titles for the same concept
    for i in range(3):
        title_session = await session_service.create_session(
            session_id=f"{session_id}_title_{i}",
            user_id=user_id,
            app_name=title_app_name
        )
        
        title_input = f"author_name: {author_name}\nstory_concept: {story_concept}"
        generated_title = await call_agent_async(
            query=title_input,
            runner=title_runner,
            user_id=user_id,
            session_id=title_session.id
        )
        
        generated_title = generated_title.strip()
        titles.append(generated_title)
        
        # Evaluate each title
        evaluation_input = f"story_concept: {story_concept}\ngenerated_title: {generated_title}"
        judge_session_id = f"{session_id}_judge_{i}"
        
        result = await evaluate_with_judge(
            judge_instruction=instruction_title_judge(),
            evaluation_input=evaluation_input,
            session_service=session_service,
            runner=judge_runner,
            user_id=user_id,
            session_id=judge_session_id,
            app_name=judge_app_name
        )
        
        scores.append(result["overall_score"])
    
    # Verify all titles meet minimum quality
    min_score = 3.0  # Slightly lower threshold for consistency test
    for i, (title, score) in enumerate(zip(titles, scores)):
        assert score >= min_score, \
            f"Title {i+1} '{title}' scored {score}, below minimum {min_score}"
    
    # Verify titles are not all identical (some variation is expected)
    unique_titles = set(titles)
    assert len(unique_titles) >= 1, "Should generate at least one unique title"
    
    print(f"\nGenerated {len(titles)} titles for concept: '{story_concept}'")
    for i, (title, score) in enumerate(zip(titles, scores), 1):
        print(f"  {i}. '{title}' - Score: {score}")

